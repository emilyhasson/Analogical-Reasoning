# Analogical Reasoning Detection Pipeline Documentation

## Overview
This project is a tool for the detection of analogical reasoning in annual reports. It consists of a multi-step pipeline with accommodation for scanned document images via OCR and for Nexis Uni multi-document downloads, though any text documents can be used with proper accomodation and cleaning.

## Prerequisites
- The pipeline is developed and tested on Windows 11 with Python 3.11.5.
- Utilizes Microsoft® Excel® for Microsoft 365 MSO (Version 2310 Build 16.0.16924.20054) 64-bit.
- Requirements and dependencies listed in requirements.txt.

## Directory Structure
- Explanation of the project directory structure.
- Purpose of each major folder and organization of files.

The project directory is organized as follows:
```plaintext
project_root/
│
├── analogy-detection/
│ ├── raw_data/
│ │ ├── dataset1.csv
│ │ ├── dataset2.csv
│ │ └── ...
│ ├── processed_data/
│ │ ├── preprocessed_dataset1.csv
│ │ ├── preprocessed_dataset2.csv
│ │ └── ...
│
├── data-generation/
│ ├── scripts/
│ │ ├── data_preprocessing.py
│ │ ├── feature_engineering.py
│ │ └── ...
│ ├── utils/
│ │ ├── helper_functions.py
│ │ └── constants.py
│ └── main.py
│
├── pdf-recognition/
│ ├── exploratory_analysis.ipynb
│ ├── model_training.ipynb
│ └── ...
│
├── text-cleaning/
│ ├── model/
│ │ ├── trained_model.pkl
│ │ └── ...
│ ├── visualizations/
│ │ ├── correlation_plot.png
│ │ └── ...
│ └── logs/
│ ├── log_file.txt
│ └── ...
│
├── README.md
├── requirements.txt
└── .gitignore
```

- **data/:** Contains raw and processed datasets.
  - `raw_data/:` Raw data files.
  - `processed_data/:` Processed data files.

- **src/:** Source code files.
  - `scripts/:` Individual scripts for data processing and analysis.
  - `utils/:` Utility functions and constants.
  - `main.py:` Main script to execute the research pipeline.

- **notebooks/:** Jupyter notebooks for exploratory analysis and modeling.

- **results/:** Output files generated by the pipeline.
  - `model/:` Saved model files.
  - `visualizations/:` Plots and visualizations.
  - `logs/:` Log files.

- **README.md:** Project documentation.

- **requirements.txt:** List of project dependencies.

## Installation
- Step-by-step instructions for setting up the environment.
- Configuration settings and any special considerations.

## Data Processing Steps

1. s
2. 

## Code Comments
- TODO add Comments within the code for clarity.
- Explanation of major functions, blocks, or operations.

## Parameters and Configurations
- List and explanation of all parameters and configurations.
- Guidelines for adjusting parameters effectively.

## Error Handling
- Documentation of potential errors and issues.
- Troubleshooting steps and solutions for common problems.

## Outputs
- Description of expected outputs at each pipeline stage.
- Sample output files or visualizations if applicable.

## Testing and Validation
- Guidelines for testing the pipeline on small datasets.
- Validation steps and expected results.

## Performance Considerations
- Discussion on scalability and performance.
- Known limitations and areas for improvement.

## Examples and Use Cases
- Practical examples demonstrating pipeline use.
- Sample commands or scripts for typical cases.

## References
- Citations and references to relevant papers or libraries.

## Contact Information
- Information for reaching out with questions or clarifications.

## Version History
- Record of changes and updates to the pipeline.

## License
- Specification of the pipeline and documentation license.

## Acknowledgments
- Recognition of contributors, libraries, or tools.

## Conclusion
- Summary of key points.
- Encouragement for feedback.

## Appendix
- Additional resources or supplementary documentation.

## Testing Documentation
- Section detailing how the pipeline has been tested.
- Information on test datasets used.
